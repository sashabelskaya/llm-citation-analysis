# llm_citation_analysis_full_grouped.py
# Dependencies: pandas, matplotlib, seaborn, networkx, urllib, numpy, scipy

import re, os
import pandas as pd
from urllib.parse import urlparse
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import numpy as np
from scipy.stats import f_oneway, kruskal

# ----------------------------
# Paths
# ----------------------------
FILE_PATH = "data/Ranklab - LLM Citation Researrch - Sheet1.csv"
OUTPUT_DIR = "outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ----------------------------
# Load data
# ----------------------------
df = pd.read_csv(FILE_PATH)

# ----------------------------
# URL extraction
# ----------------------------
url_text_cols = [c for c in ["cited_url", "... links", "cited_urls", "links", "response", "answer", "text"] if c in df.columns]
url_re = re.compile(r"https?://[^\s)>\"]+")

def extract_urls(text):
    if not isinstance(text, str): return []
    urls = url_re.findall(text)
    return [u.rstrip(".,;:')]>\"") for u in urls]

def gather_urls_from_row(row):
    urls = []
    for c in url_text_cols:
        v = row.get(c, "")
        if isinstance(v, str) and v.strip():
            if v.startswith("http://") or v.startswith("https://"):
                parts = re.split(r"[\s,;]+", v)
                for p in parts:
                    if p.startswith("http://") or p.startswith("https://"):
                        urls.append(p.rstrip(".,;:')]>\""))
            else:
                urls.extend(extract_urls(v))
    seen = set(); out = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out

# ----------------------------
# Group by prompt and system
# ----------------------------
group_cols = ["prompt"]
if "system" in df.columns:
    group_cols.append("system")

grouped = df.groupby(group_cols, as_index=False).agg({
    **{c: "first" for c in df.columns if c not in group_cols},
})

# ----------------------------
# Use n. of links if available, else extract URLs
# ----------------------------
if "n. of links" in grouped.columns:
    grouped["_num_urls"] = pd.to_numeric(grouped["n. of links"], errors='coerce').fillna(0).astype(int)
else:
    grouped["_extracted_urls"] = grouped.apply(gather_urls_from_row, axis=1)
    grouped["_num_urls"] = grouped["_extracted_urls"].apply(len)

# ----------------------------
# Domain and TLD extraction
# ----------------------------
def domain_from_url(url):
    try:
        p = urlparse(url)
        domain = p.netloc.lower().split(':')[0]
        if domain.startswith("www."):
            domain = domain[4:]
        return domain
    except:
        return None

if "_extracted_urls" not in grouped.columns:
    grouped["_extracted_urls"] = grouped.apply(gather_urls_from_row, axis=1)

grouped["_domains"] = grouped["_extracted_urls"].apply(lambda urls: [domain_from_url(u) for u in urls])
exploded = grouped.explode("_domains").rename(columns={"_domains":"domain_exploded"})
exploded = exploded[~exploded["domain_exploded"].isna() & (exploded["domain_exploded"]!="")]
exploded["_tld"] = exploded["domain_exploded"].apply(lambda d: ("." + d.split(".")[-1]) if isinstance(d, str) and "." in d else "other")

# ----------------------------
# Overall Metrics
# ----------------------------
metrics = {
    "total_responses": len(grouped),
    "responses_with_links": int((grouped["_num_urls"]>0).sum()),
    "total_extracted_links": int(grouped["_num_urls"].sum()),
    "avg_links_per_response": float(grouped["_num_urls"].mean()),
    "median_links_per_response": float(grouped["_num_urls"].median()),
    "unique_domains": int(exploded["domain_exploded"].nunique())
}

print("=== Overall Metrics ===")
metrics_df = pd.DataFrame(list(metrics.items()), columns=["Metric","Value"])
display(metrics_df)

# ----------------------------
# Top 10 Domains
# ----------------------------
print("\n=== Top 10 Domains ===")
domain_counts = exploded["domain_exploded"].value_counts().head(10).reset_index()
domain_counts.columns = ["Domain","Count"]
display(domain_counts)

# ----------------------------
# TLD distribution
# ----------------------------
tld_counts = exploded["_tld"].value_counts().reset_index()
tld_counts.columns = ["TLD","Count"]
tld_counts["Pct"] = tld_counts["Count"] / tld_counts["Count"].sum() * 100
print("\n=== TLD Distribution ===")
display(tld_counts)

# ----------------------------
# Avg links per system
# ----------------------------
if "system" in grouped.columns:
    avg_by_system = grouped.groupby("system")["_num_urls"].agg(["count","mean","median"]).sort_values("mean", ascending=False)
    print("\n=== Avg Links per System ===")
    display(avg_by_system)

    plt.figure(figsize=(8,4))
    plt.bar(avg_by_system.index.astype(str), avg_by_system["mean"])
    plt.xticks(rotation=45, ha='right')
    plt.title("Average # links per response by system")
    plt.tight_layout()
    plt.show()

# ----------------------------
# Pie chart for TLDs
# ----------------------------
tld_summary = exploded["_tld"].value_counts()
tld_plot_vals = [
    tld_summary.get(".com",0),
    tld_summary.get(".org",0),
    tld_summary.get(".edu",0),
    tld_summary.get(".gov",0),
    tld_summary.sum() - sum([tld_summary.get(x,0) for x in [".com",".org",".edu",".gov"]])
]
tld_plot_labels = [".com",".org",".edu",".gov","other"]
plt.figure(figsize=(12,12))
plt.pie(tld_plot_vals, labels=tld_plot_labels, autopct="%1.1f%%")
plt.title("TLD Distribution")
plt.show()

# ----------------------------
# Network graph: Systems <-> Top 50 Domains
# ----------------------------
top_dom_list = exploded["domain_exploded"].value_counts().head(50).index.tolist()
systems = grouped["system"].dropna().unique().tolist()
G = nx.Graph()
G.add_nodes_from(systems, bipartite=0)
G.add_nodes_from(top_dom_list, bipartite=1)

for idx, row in grouped.iterrows():
    s = row.get("system", "unknown")
    for d in row["_domains"]:
        if d in top_dom_list:
            G.add_edge(s, d)

plt.figure(figsize=(14,14))
pos = nx.spring_layout(G, k=0.6, seed=42)
nx.draw_networkx_nodes(G, pos, nodelist=systems, node_shape='s', node_size=600)
nx.draw_networkx_nodes(G, pos, nodelist=top_dom_list, node_shape='o', node_size=200)
nx.draw_networkx_edges(G, pos, alpha=0.3)
nx.draw_networkx_labels(G, pos, font_size=8)
plt.axis('off')
plt.title("Systems <-> Top 50 Domains")
plt.show()

# ----------------------------
# Save processed files
# ----------------------------
grouped.to_csv(f"{OUTPUT_DIR}/processed_responses_with_urls.csv", index=False)
exploded.to_csv(f"{OUTPUT_DIR}/exploded_domain_rows.csv", index=False)
print("Saved processed outputs and visualizations.")

# ----------------------------
# Niche Analysis: Stats & Visualizations
# ----------------------------
if "niche" not in exploded.columns:
    exploded["niche"] = "other"

# Top domains per niche
domain_by_niche = exploded.groupby(["niche", "domain_exploded"]).size().reset_index(name="count")
top_domains_per_niche = domain_by_niche.sort_values(["niche","count"], ascending=[True,False]).groupby("niche").head(10)
print("=== Top Domains per Niche ===")
display(top_domains_per_niche)

# TLD distribution per niche
tld_by_niche = exploded.groupby(["niche","_tld"]).size().reset_index(name="count")
tld_by_niche["pct"] = tld_by_niche.groupby("niche")["count"].transform(lambda x: x / x.sum() * 100)
print("=== TLD Distribution per Niche ===")
display(tld_by_niche)

# Avg links per niche
avg_links_per_niche = grouped.groupby("niche")["_num_urls"].agg(["count","mean","median"]).sort_values("mean", ascending=False)
print("=== Avg Links per Niche ===")
display(avg_links_per_niche)

# Bar charts: Top domains per niche
for niche in exploded["niche"].unique():
    data = top_domains_per_niche[top_domains_per_niche["niche"]==niche]
    if data.empty: continue
    plt.figure(figsize=(10,5))
    sns.barplot(data=data, x="domain_exploded", y="count", palette="viridis")
    plt.xticks(rotation=45, ha='right')
    plt.title(f"Top Domains in Niche: {niche}")
    plt.ylabel("Count")
    plt.xlabel("Domain")
    plt.tight_layout()
    plt.show()

# Pie charts: TLD distribution per niche
for niche in exploded["niche"].unique():
    data = tld_by_niche[tld_by_niche["niche"]==niche]
    if data.empty: continue
    plt.figure(figsize=(10,10))
    plt.pie(data["count"], labels=data["_tld"], autopct="%1.1f%%", startangle=140)
    plt.title(f"TLD Distribution in Niche: {niche}")
    plt.show()

# ----------------------------
# Part 3: Enhanced Content Format Analysis
# ----------------------------

# Normalize column names (safe: after all previous analyses)
exploded.columns = exploded.columns.str.strip().str.lower()

# Identify content format column robustly
content_col = next((c for c in exploded.columns if "content" in c and "format" in c), None)
if content_col is None:
    print("No content format column found. Skipping content format analysis.")
else:
    exploded = exploded.rename(columns={content_col: "content_format"})

    # Clean content_format values
    exploded["content_format"] = exploded["content_format"].astype(str).str.strip().str.title()

    # Exclude broken / unknown links
    broken_links = exploded[exploded["content_format"].isin(["Nan", "Unknown", "", "N/A", "None"])]
    exploded = exploded[~exploded.index.isin(broken_links.index)]
    broken_links.to_csv(f"{OUTPUT_DIR}/broken_links.csv", index=False)

    # Normalize per prompt
    if "prompt" in exploded.columns:
        prompt_cf_counts = exploded.groupby(["prompt", "content_format"]).size().reset_index(name="count")
        prompt_total_counts = exploded.groupby("prompt").size().reset_index(name="total_links")
        prompt_cf_counts = prompt_cf_counts.merge(prompt_total_counts, on="prompt")
        prompt_cf_counts["pct_per_prompt"] = prompt_cf_counts["count"] / prompt_cf_counts["total_links"] * 100
        prompt_cf_counts.to_csv(f"{OUTPUT_DIR}/content_format_per_prompt.csv", index=False)

    # Overall distribution
    overall_cf = exploded["content_format"].value_counts().reset_index()
    overall_cf.columns = ["content_format", "count"]
    overall_cf["pct"] = overall_cf["count"] / overall_cf["count"].sum() * 100
    print("=== Overall Content Format Distribution ===")
    display(overall_cf)

    # Distribution per system
    if "system" in exploded.columns:
        system_cf = exploded.groupby(["system", "content_format"]).size().reset_index(name="count")
        system_cf["pct"] = system_cf.groupby("system")["count"].transform(lambda x: x / x.sum() * 100)
        if "_num_urls" in exploded.columns:
            system_stats = exploded.groupby("system")["_num_urls"].agg(["count","mean","median","std"])
            system_stats["IQR"] = exploded.groupby("system")["_num_urls"].apply(lambda x: np.percentile(x,75)-np.percentile(x,25))
        else:
            system_stats = None
        print("=== Content Format Distribution per System ===")
        display(system_cf)
        if system_stats is not None:
            print("=== Statistical Metrics per System ===")
            display(system_stats)

    # Distribution per niche
    if "niche" in exploded.columns:
        niche_cf = exploded.groupby(["niche","content_format"]).size().reset_index(name="count")
        niche_cf["pct"] = niche_cf.groupby("niche")["count"].transform(lambda x: x / x.sum() * 100)
        if "_num_urls" in exploded.columns:
            niche_stats = exploded.groupby("niche")["_num_urls"].agg(["count","mean","median","std"])
            niche_stats["IQR"] = exploded.groupby("niche")["_num_urls"].apply(lambda x: np.percentile(x,75)-np.percentile(x,25))
        else:
            niche_stats = None
        print("=== Content Format Distribution per Niche ===")
        display(niche_cf)
        if niche_stats is not None:
            print("=== Statistical Metrics per Niche ===")
            display(niche_stats)

    # Distribution by TLD
    if "_tld" in exploded.columns:
        tld_cf = exploded.groupby(["_tld","content_format"]).size().reset_index(name="count")
        tld_cf["pct"] = tld_cf.groupby("_tld")["count"].transform(lambda x: x / x.sum() * 100)
        print("=== Content Format Distribution per TLD ===")
        display(tld_cf)

    # Statistical tests
    if "system" in exploded.columns and "_num_urls" in exploded.columns:
        systems = exploded["system"].unique()
        link_lists = [exploded.loc[exploded["system"]==s, "_num_urls"] for s in systems]
        link_lists = [lst for lst in link_lists if len(lst) > 1]
        if len(link_lists) > 1:
            anova_result = f_oneway(*link_lists)
            kruskal_result = kruskal(*link_lists)
            print("=== Statistical Tests: Links per System ===")
            print(f"ANOVA: F={anova_result.statistic:.3f}, p={anova_result.pvalue:.3f}")
            print(f"Kruskal-Wallis: H={kruskal_result.statistic:.3f}, p={kruskal_result.pvalue:.3f}")
        else:
            print("Not enough data for ANOVA/Kruskal-Wallis tests.")

    # Visualizations
    plt.figure(figsize=(12,6))
    sns.barplot(data=overall_cf, x="content_format", y="count", palette="viridis")
    plt.xticks(rotation=45, ha='right')
    plt.title("Overall Content Format Distribution")
    plt.ylabel("Count")
    plt.xlabel("Content Format")
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/overall_content_format.png")
    plt.close()

    if "system" in exploded.columns:
        for sys in exploded["system"].unique():
            data = system_cf[system_cf["system"] == sys]
            plt.figure(figsize=(12,6))
            sns.barplot(data=data, x="content_format", y="count", palette="coolwarm")
            plt.xticks(rotation=45, ha='right')
            plt.title(f"Content Format Distribution for System: {sys}")
            plt.ylabel("Count")
            plt.xlabel("Content Format")
            plt.tight_layout()
            plt.savefig(f"{OUTPUT_DIR}/content_format_{sys}.png")
            plt.close()

    if "niche" in exploded.columns:
        for niche in exploded["niche"].unique():
            data = niche_cf[niche_cf["niche"] == niche]
            plt.figure(figsize=(12,6))
            sns.barplot(data=data, x="content_format", y="count", palette="magma")
            plt.xticks(rotation=45, ha='right')
            plt.title(f"Content Format Distribution in Niche: {niche}")
            plt.ylabel("Count")
            plt.xlabel("Content Format")
            plt.tight_layout()
            plt.savefig(f"{OUTPUT_DIR}/content_format_{niche}.png")
            plt.close()

    if "_tld" in exploded.columns:
        for tld in exploded["_tld"].unique():
            data = tld_cf[tld_cf["_tld"] == tld]
            plt.figure(figsize=(8,8))
            plt.pie(data["count"], labels=data["content_format"], autopct="%1.1f%%", startangle=140)
            plt.title(f"Content Format Distribution for TLD: {tld}")
            plt.savefig(f"{OUTPUT_DIR}/content_format_{tld}.png")
            plt.close()

    # Save CSVs
    overall_cf.to_csv(f"{OUTPUT_DIR}/content_format_overall.csv", index=False)
    if "system" in exploded.columns:
        system_cf.to_csv(f"{OUTPUT_DIR}/content_format_per_system.csv", index=False)
        if system_stats is not None:
            system_stats.to_csv(f"{OUTPUT_DIR}/system_stats.csv")
    if "niche" in exploded.columns:
        niche_cf.to_csv(f"{OUTPUT_DIR}/content_format_per_niche.csv", index=False)
        if niche_stats is not None:
            niche_stats.to_csv(f"{OUTPUT_DIR}/niche_stats.csv")
    if "_tld" in exploded.columns:
        tld_cf.to_csv(f"{OUTPUT_DIR}/content_format_per_tld.csv", index=False)

    print("Saved enhanced content format analysis outputs (broken links excluded).")
